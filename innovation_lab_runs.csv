id,timestamp,preset,dsl_code,epochs,final_loss,final_val_loss,accuracy,f1,suggestions,notes,model_path,loss_fn,lr
1,2026-02-18T21:31:01.904004Z,Classifier (MLP),"[128, 64], dropout: [0.2], [64, 32], [32, 10]",5,0.45562878251075745,0.80789715051651,0.0,0.0,YOUR ARCHITECTURE LOOKS OPTIMAL: Proceed with training.,AutoResearcher fact: Adding RMSNorm before MoE routing can stabilize training in deep architectures.,,MSE,0.01
1,2026-02-18T21:31:26.149533Z,Research Frontier,"kan: [128], diff_attn: [128], lora: [128, 16], [128, 10]",15,0.1706565022468567,0.698982834815979,0.0,0.0,RESEARCH TIP: Consider replacing 'attn' with 'mamba' for linear-time complexity if sequence length is high.,AutoResearcher fact: Mamba layers are more efficient for sequence lengths > 2000 compared to standard Attention.,,MSE,0.01
2,2026-02-18T21:31:58.038524Z,Research Frontier,"kan: [128], diff_attn: [128], lora: [128, 16], [128, 10]",15,0.17146822810173035,0.7210109829902649,0.0,0.0,RESEARCH TIP: Consider replacing 'attn' with 'mamba' for linear-time complexity if sequence length is high.,AutoResearcher fact: Adding RMSNorm before MoE routing can stabilize training in deep architectures.,,MSE,0.01
